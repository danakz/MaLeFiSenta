{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "import string\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import os\n",
    "import glob\n",
    "from matplotlib import cm\n",
    "from PIL import Image, ImageOps\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Auxiliary functions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def gaussian2(size, sigma):\n",
    "    \"\"\"Returns a normalized circularly symmetric 2D gauss kernel array\n",
    "    \n",
    "    f(x,y) = A.e^{-(x^2/2*sigma^2 + y^2/2*sigma^2)} where\n",
    "    \n",
    "    A = 1/(2*pi*sigma^2)\n",
    "    \n",
    "    as define by Wolfram Mathworld \n",
    "    http://mathworld.wolfram.com/GaussianFunction.html\n",
    "    \"\"\"\n",
    "    A = 1/(2.0*numpy.pi*sigma**2)\n",
    "    x, y = numpy.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
    "    g = A*numpy.exp(-((x**2/(2.0*sigma**2))+(y**2/(2.0*sigma**2))))\n",
    "    return g\n",
    "\n",
    "def fspecial_gauss(size, sigma):\n",
    "    \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function\n",
    "    \"\"\"\n",
    "    x, y = numpy.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
    "    g = numpy.exp(-((x**2 + y**2)/(2.0*sigma**2)))\n",
    "    return g/g.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "from scipy import signal\n",
    "from scipy import ndimage\n",
    "def ssim(img1, img2, cs_map=False):\n",
    "    \"\"\"Return the Structural Similarity Map corresponding to input images img1 \n",
    "    and img2 (images are assumed to be uint8)\n",
    "    \n",
    "    This function attempts to mimic precisely the functionality of ssim.m a \n",
    "    MATLAB provided by the author's of SSIM\n",
    "    https://ece.uwaterloo.ca/~z70wang/research/ssim/ssim_index.m\n",
    "    \"\"\"\n",
    "    img1 = img1.astype(numpy.float64)\n",
    "    img2 = img2.astype(numpy.float64)\n",
    "    size = 11\n",
    "    sigma = 1.5\n",
    "    window = fspecial_gauss(size, sigma)\n",
    "    K1 = 0.01\n",
    "    K2 = 0.03\n",
    "    L = 255 #bitdepth of image\n",
    "    C1 = (K1*L)**2\n",
    "    C2 = (K2*L)**2\n",
    "    mu1 = signal.fftconvolve(window, img1, mode='valid')\n",
    "    mu2 = signal.fftconvolve(window, img2, mode='valid')\n",
    "    mu1_sq = mu1*mu1\n",
    "    mu2_sq = mu2*mu2\n",
    "    mu1_mu2 = mu1*mu2\n",
    "    sigma1_sq = signal.fftconvolve(window, img1*img1, mode='valid') - mu1_sq\n",
    "    sigma2_sq = signal.fftconvolve(window, img2*img2, mode='valid') - mu2_sq\n",
    "    sigma12 = signal.fftconvolve(window, img1*img2, mode='valid') - mu1_mu2\n",
    "    if cs_map:\n",
    "        return (((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*\n",
    "                    (sigma1_sq + sigma2_sq + C2)), \n",
    "                (2.0*sigma12 + C2)/(sigma1_sq + sigma2_sq + C2))\n",
    "    else:\n",
    "        return ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*\n",
    "                    (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "def msssim(img1, img2):\n",
    "    \"\"\"This function implements Multi-Scale Structural Similarity (MSSSIM) Image \n",
    "    Quality Assessment according to Z. Wang's \"Multi-scale structural similarity \n",
    "    for image quality assessment\" Invited Paper, IEEE Asilomar Conference on \n",
    "    Signals, Systems and Computers, Nov. 2003 \n",
    "    \n",
    "    Author's MATLAB implementation:-\n",
    "    http://www.cns.nyu.edu/~lcv/ssim/msssim.zip\n",
    "    \"\"\"\n",
    "    level = 5\n",
    "    weight = numpy.array([0.0448, 0.2856, 0.3001, 0.2363, 0.1333])\n",
    "    downsample_filter = numpy.ones((2, 2))/4.0\n",
    "    im1 = img1.astype(numpy.float64)\n",
    "    im2 = img2.astype(numpy.float64)\n",
    "    mssim = numpy.array([])\n",
    "    mcs = numpy.array([])\n",
    "    for l in range(level):\n",
    "        ssim_map, cs_map = ssim (im1, im2, cs_map=True)\n",
    "        mssim = numpy.append (mssim, ssim_map.mean())\n",
    "        mcs = numpy.append (mcs, cs_map.mean())\n",
    "        filtered_im1 = ndimage.filters.convolve(im1, downsample_filter, \n",
    "                                                mode='reflect')\n",
    "        filtered_im2 = ndimage.filters.convolve(im2, downsample_filter, \n",
    "                                                mode='reflect')\n",
    "        im1 = filtered_im1[::2, ::2]\n",
    "        im2 = filtered_im2[::2, ::2]\n",
    "    #print (mcs[0:level-1]**2)\n",
    "    #print (weight[0:level-1]) \n",
    "    return (numpy.prod(mcs[0:level-1]**weight[0:level-1])*\n",
    "                    (mssim[level-1]**weight[level-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def make_grid (count, index, cols):\n",
    "    if (count >= 10):\n",
    "        #index_col = (index % cols) + 1\n",
    "        #index_row = (index / cols) + 1\n",
    "        total_rows = math.ceil(count / cols)\n",
    "        total_cols = cols\n",
    "        #options = str (total_rows) + str (total_cols) + \n",
    "        return total_rows,total_cols\n",
    "    else:\n",
    "        total_rows = math.ceil(count / cols)\n",
    "        total_cols = cols\n",
    "        #options = str (total_rows) + str (total_cols) + \n",
    "        return total_rows,total_cols    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_plot_images (original_image, images):\n",
    "    maps = []\n",
    "    original_data = numpy.asarray (original_image)\n",
    "    fil_data = []\n",
    "    import pylab\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        for image in images:    \n",
    "            img = numpy.asarray(image)\n",
    "            fil_data.append (img)\n",
    "    except Exception (e):\n",
    "        e = 'Cannot load images' + str(e)\n",
    "        print >> sys.stderr, e\n",
    "    ssimRes = []\n",
    "    msSimRes = []\n",
    "    best_index = -1\n",
    "    best = 0\n",
    "    for ind, d in enumerate(fil_data):\n",
    "        original_data = (original_data/original_data.max())*255\n",
    "        d = (d/d.max()) * 255\n",
    "        ssim_map = ssim(original_data, d)\n",
    "        ms_ssim = msssim(original_data, d)\n",
    "        if (ms_ssim > best):\n",
    "            best = ms_ssim\n",
    "            best_index = ind\n",
    "        #print (original_data [original_data != 0].shape)\n",
    "        ssimRes.append (ssim_map)\n",
    "        msSimRes.append (ms_ssim)\n",
    "    pylab.figure(figsize=(8, 6), dpi=300)\n",
    "    #options = '1' + str (len (paths)) + '1'\n",
    "    r, c = make_grid (len (images) + 1, 1, 3)\n",
    "    #print (r, ' ', c)\n",
    "    pylab.subplot(r, c , 1)\n",
    "    plt.axis('off')\n",
    "    pylab.title('Original')\n",
    "    pylab.imshow(original_data, interpolation='nearest', cmap = 'RdBu_r')\n",
    "    for i in range (len (images)):    \n",
    "        #options = '1' + str (len (paths)) + str (i + 2)\n",
    "        pylab.subplot(r, c, i + 2)\n",
    "        pylab.imshow(fil_data [i], interpolation='nearest', cmap = 'RdBu_r')\n",
    "        #pylab.subplot(133)\n",
    "        s = \"{value:.4f}\"\n",
    "        s = s.format(value = msSimRes[i])\n",
    "        title = s\n",
    "        pylab.title(s)\n",
    "        #pylab.title('SSIM Map\\n SSIM: %f\\n MSSSIM: %f' % (ssimRes[i].mean(), msSimRes[i]))\n",
    "        plt.axis('off')\n",
    "        #pylab.imshow(ssim_map, interpolation='nearest')\n",
    "    rand_ind = np.random.randint(1000000)    \n",
    "    filename = original_path.split ('/')[-1].replace ('.fits','.png')\n",
    "    filename = str (rand_ind) + '.png'\n",
    "    pylab.savefig('datasets/msim_paper2/' + filename,bbox_inches='tight',pad_inches=0)\n",
    "    return best_index\n",
    "    #pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_plot (original_path, paths, options):\n",
    "    maps = []\n",
    "    original_data = fits.open (original_path)\n",
    "    original_data = numpy.asarray (original_data[0].data)\n",
    "    fil_data = []\n",
    "    import pylab\n",
    "    try:\n",
    "        from PIL import Image\n",
    "        for path in paths:    \n",
    "            data = fits.open(path)\n",
    "            fil_map = data[0].data\n",
    "            img = numpy.asarray(fil_map)\n",
    "            fil_data.append (fil_map)\n",
    "    except Exception (e):\n",
    "        e = 'Cannot load images' + str(e)\n",
    "        print >> sys.stderr, e\n",
    "    ssimRes = []\n",
    "    msSimRes = []\n",
    "    best_index = -1\n",
    "    best = 0\n",
    "    for ind, d in enumerate(fil_data):\n",
    "        original_data = (original_data/original_data.max())*255\n",
    "        d = (d/d.max()) * 255\n",
    "        ssim_map = ssim(original_data, d)\n",
    "        ms_ssim = msssim(original_data, d)\n",
    "        if (ms_ssim > best):\n",
    "            best = ms_ssim\n",
    "            best_index = ind\n",
    "    return best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_msim_maps (): \n",
    "    i = 0\n",
    "    best_msims = []\n",
    "    for key, v in filaments.items ():\n",
    "        original_path = 'datasets/colden/' + key + '.fits'     \n",
    "        items = [k for k,v1 in v]\n",
    "        mask = fits.open (items [1])\n",
    "        opts = [v1 for k,v1 in v]\n",
    "        best_msims.append (ssim_plot (original_path, items, opts))\n",
    "        i+=1\n",
    "    return best_msims  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> reading fits files </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Reading Planck dataset </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Reading Planck-1 dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from matplotlib import cm\n",
    "from PIL import Image, ImageOps\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "import tensorflow as tf\n",
    "images_list = [f for f in glob.glob(\"datasets/Pdata/*_I_*\")]\n",
    "rht_list = [f for f in glob.glob(\"datasets/Pdata/*_SUPERHT_THETA_*\")]\n",
    "mask_list = [f for f in glob.glob(\"datasets/Pdata/*_SUPERHT_G*\")]\n",
    "images_all = []\n",
    "angles_all = []\n",
    "masks_all = []\n",
    "pil_all =  []\n",
    "for index_img, img in enumerate (images_list):\n",
    "    image_f = fits.open(img)\n",
    "    image_d = image_f [0].data\n",
    "    newSize = (128,128)\n",
    "    image_d = Image.fromarray((image_d / image_d.max()) * 255)\n",
    "    image_d = image_d.resize(newSize)\n",
    "    image_d = np.array (image_d)\n",
    "    \n",
    "    ind = img.index ('_I_')\n",
    "    angles_file = img[:ind] + '_SUPERHT_THETA_' + img [ind + 3:]\n",
    "    mask_file = img[:ind] + '_SUPERHT_' + img [ind + 3:]\n",
    "    print (angles_file)\n",
    "    angles = fits.open(angles_file)\n",
    "    angles = angles[0].data\n",
    "    fils = fits.open(mask_file)\n",
    "    fils = fils[0].data\n",
    "    \n",
    "    angles = Image.fromarray(angles)\n",
    "    angles = angles.resize(newSize)\n",
    "    angles = np.array (angles)\n",
    "    \n",
    "    fils = np.array (fils)\n",
    "    fils = Image.fromarray((fils / fils.max()) * 255)\n",
    "    fils = fils.resize(newSize)\n",
    "    \n",
    "    images_all.append (np.array (image_d))\n",
    "    angles_all.append (np.array (angles))\n",
    "    masks_all.append (np.array (fils))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Reading Planck-cc dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from matplotlib import cm\n",
    "from PIL import Image, ImageOps\n",
    "from skimage import data, color\n",
    "import subprocess\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "def find_masks_and_images_best ():\n",
    "    images_all = []\n",
    "    masks_all = []\n",
    "    pil_all =  []\n",
    "    i = 0\n",
    "    filament_names = list (set ([d.split ('\\\\')[1].split ('lon')[-1][0:-5].replace ('_smr27_wlen7','') for d in glob.glob ('cc_fil/BEST/*')]))\n",
    "    root = os.getcwd().replace ('\\\\','/') + '/'\n",
    "    for filament in filament_names:\n",
    "        orig_path = root + 'cc_fil/BEST/_I_g_lon' + filament + '.fits'\n",
    "        angle_path = root + 'cc_fil/BEST/theta/suprht_theta_lon-' + filament + '_K27_BAR7.fits'\n",
    "        angle_path2 = root + 'cc_fil/BEST/theta/suprht_theta_lon' + filament + '_K27_BAR7.fits'\n",
    "        if (os.path.exists (angle_path) == False):\n",
    "            if (os.path.exists (angle_path2) == False):\n",
    "                continue\n",
    "            else:\n",
    "                angle_path = angle_path2\n",
    "        \n",
    "        if (os.path.exists (orig_path) == False):\n",
    "            continue    \n",
    "        mask = fits.open (angle_path)\n",
    "        image_f = fits.open (orig_path)\n",
    "        print (angle_path)\n",
    "        image_d = np.array (image_f[0].data).astype('float32')\n",
    "        image_dd = (image_d / image_d.max()) *  255.0;\n",
    "        image_d = Image.fromarray(image_dd)\n",
    "        newSize = (128,128)\n",
    "        image_p = image_d.resize(newSize)\n",
    "        image_f = np.array (image_p).astype ('float32')\n",
    "        image_f = (image_f / image_f.max()) * 255.0;\n",
    "        image_f = image_f.clip(min = 0)\n",
    "        image_dd = image_f[image_f != 0]\n",
    "        mask_d = np.array (mask[0].data)\n",
    "        mask_p = Image.fromarray(mask_d)\n",
    "        newSize = (128,128)\n",
    "        mask_p = mask_p.resize(newSize)\n",
    "        mask = np.array (mask_p)\n",
    "        mask = mask.tolist()\n",
    "        masks_all.append (mask)\n",
    "        images_all.append (image_f)\n",
    "        i+=1\n",
    "    return images_all, masks_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Herschel dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "dirs = [d.split ('\\\\')[1] for d in glob.glob ('SUPRHT/OUTPUT*COLD_DEN')]\n",
    "print (dirs)\n",
    "filaments = {}\n",
    "for d in dirs:\n",
    "    cur_folder = 'SUPRHT/' + d\n",
    "    files = [f.split ('\\\\')[1] for f in glob.glob (cur_folder+'/*') if f.count ('THETA') == 0]\n",
    "    for file in files:\n",
    "        fil = file.split ('_SUPRHT_')[0]\n",
    "        if (fil in filaments):\n",
    "            path = cur_folder + '/' + file\n",
    "            opt = file.split ('_') [-2:]\n",
    "            opt[1] = opt[1].replace ('.fits','')\n",
    "            opt = ' '.join (opt)\n",
    "            data = fits.open(path)\n",
    "            filaments[fil].append ((path,opt))\n",
    "        else:\n",
    "            path = cur_folder + '/' + file\n",
    "            data = fits.open(path)\n",
    "            fil_map = data[0].data \n",
    "            filaments[fil] = []\n",
    "            opt = file.split ('_') [-2:]\n",
    "            opt[1] = opt[1].replace ('.fits','')\n",
    "            opt = ' '.join (opt)\n",
    "            filaments[fil].append ((path,opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> FInding best parameters of RHT with MSSIM for each Herschel GCC map </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from matplotlib import cm\n",
    "from PIL import Image, ImageOps \n",
    "from skimage import data, color\n",
    "import subprocess\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "def find_hershel ():\n",
    "    images_all = []\n",
    "    masks_all = []\n",
    "    pil_all =  []\n",
    "    fmasks = []\n",
    "    i = 0\n",
    "    for key, v in filaments2.items ():\n",
    "        original_path = 'datasets/colden/' + key + '.fits'     \n",
    "        items = [k for k,v1 in v]\n",
    "        angles_file = filaments [key][best_msims[i]][0]\n",
    "        angle_fits = fits.open (angles_file)\n",
    "        mask_fits = fits.open (items [best_msims[i]])\n",
    "        fname = items [best_msims[i]].split('/')[-1]\n",
    "        opts = [v1 for k,v1 in v]\n",
    "        image_f = fits.open (original_path)\n",
    "        print (original_path)\n",
    "        image_d = np.array (image_f[0].data).astype('float32')\n",
    "        image_dd = (image_d / image_d.max()) *  255.0;\n",
    "        image_d = Image.fromarray(image_dd)\n",
    "        newSize = (128,128)\n",
    "        image_p = image_d.resize(newSize)\n",
    "        image_f = np.array (image_p).astype ('float32')\n",
    "        image_f = (image_f / image_f.max()) * 255.0;\n",
    "        image_f = image_f.clip(min = 0)\n",
    "        image_dd = image_f[image_f != 0]\n",
    "        angles_d = np.array (angle_fits[0].data)\n",
    "        mask_p = Image.fromarray(angles_d)\n",
    "        mask_p = mask_p.resize(newSize)\n",
    "        mask = np.array (mask_p)\n",
    "        \n",
    "        mask_d = np.array (mask_fits[0].data)\n",
    "        mask_d = Image.fromarray(mask_d)\n",
    "        mask_p = mask_d.resize(newSize)\n",
    "        mask_real = np.array (mask_p)\n",
    "        masks_all.append (mask)\n",
    "        fmasks.append (mask_real)\n",
    "        images_all.append (image_f)\n",
    "        i+=1\n",
    "    return images_all, masks_all, fmasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_msims = find_best_msim_maps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_all,angles_all,masks_all = find_hershel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Reading HI dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from matplotlib import cm\n",
    "from PIL import Image, ImageOps\n",
    "from skimage import data, color\n",
    "import subprocess\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "def find_h ():\n",
    "    images_all1 = []\n",
    "    masks_all1 = []\n",
    "    pil_all =  []\n",
    "    angles_all1 = []\n",
    "    i = 0\n",
    "    filament_names = list (set ([d.split ('\\\\')[1].split ('orig_lon')[1][0:-5].replace ('_K27_BAR7','').replace ('_smr27_wlen7','') for d in glob.glob ('datasets/Hdata/*orig_lon*')]))\n",
    "    root = os.getcwd().replace ('\\\\','/') + '/'\n",
    "    for filament in filament_names:\n",
    "        filament.replace ('lat-','lat')\n",
    "        mask_path = root + 'datasets/Hdata/suprht_theta_lon' + filament + '_K27_BAR7.fits'\n",
    "        orig_path = root + 'datasets/Hdata/orig_lon' + filament + '.fits'\n",
    "        mask2_path = root + 'datasets/Hdata/suprht_lon' + filament + '_K27_BAR7.fits'\n",
    "        print (mask_path)\n",
    "        if (os.path.exists (mask_path) == False):\n",
    "            continue\n",
    "        if (os.path.exists (orig_path) == False):\n",
    "            continue\n",
    "        if (os.path.exists (mask2_path) == False):\n",
    "            print ('not found')\n",
    "            continue\n",
    "        mask = fits.open (mask_path)\n",
    "        image_f = fits.open (orig_path)\n",
    "        mask2 = fits.open (mask2_path)\n",
    "        mask2 = np.array (mask2[0].data)\n",
    "        image_d = np.array (image_f[0].data).astype('float32')\n",
    "        image_dd = (image_d / image_d.max()) *  255.0;\n",
    "        image_d = Image.fromarray(image_dd)\n",
    "        newSize = (128,128)\n",
    "        image_p = image_d.resize(newSize)\n",
    "        image_f = np.array (image_p).astype ('float32')\n",
    "        image_f = (image_f / image_f.max()) * 255.0;\n",
    "        image_f = image_f.clip(min = 0)\n",
    "        image_dd = image_f[image_f != 0]\n",
    "        mask_d = np.array (mask[0].data)\n",
    "        mask_p = Image.fromarray(mask_d)\n",
    "        newSize = (128,128)\n",
    "        mask_p = mask_p.resize(newSize)\n",
    "        mask = np.array (mask_p)\n",
    "        mask2 = Image.fromarray(mask2)\n",
    "        newSize = (128,128)\n",
    "        mask2 = mask2.resize(newSize)\n",
    "        mask2 = np.array (mask2)\n",
    "        masks_all1.append (mask2)\n",
    "        images_all1.append (image_f)\n",
    "        angles_all1.append (mask)\n",
    "        i+=1\n",
    "    return images_all1, masks_all1, angles_all1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images_allH, masks_allH, angles_allH = find_h ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inmaps, anglemaps = find_masks_and_images_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Dataset preparation </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Preparing input data with Planck-1 / Planck-cc / Herschel datasets </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images_all = np.array (images_all, dtype = np.float)\n",
    "angles_all = np.array (angles_all, dtype=np.float)\n",
    "masks_all = np.array (masks_all, dtype=np.float)\n",
    "y_train2 = np.zeros ((images_all.shape[0] , images_all.shape[1],images_all.shape[2]))\n",
    "print (y_train2.shape)\n",
    "for i in range (images_all.shape[0]):\n",
    "    y_train2 [i,:] = np.array(angles_all [i])\n",
    "x_train = images_all[0:].reshape ((images_all.shape[0],images_all.shape[1],images_all.shape[2],1))\n",
    "y_train = np.expand_dims (y_train2,axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Preparing input data with HI dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_allH = np.array (images_allH, dtype = np.float)\n",
    "angles_allH = np.array (angles_allH, dtype=np.float)\n",
    "masks_allH = np.array (masks_allH, dtype=np.float)\n",
    "y_train2 = np.zeros ((images_allH.shape[0] , images_allH.shape[1],images_allH.shape[2]))\n",
    "for i in range (images_allH.shape[0]):\n",
    "    y_train2 [i,:] = np.array(angles_allH [i])\n",
    "x_train = masks_allH[0:].reshape ((images_allH.shape[0],images_allH.shape[1],images_allH.shape[2],1))\n",
    "y_train = np.expand_dims (y_train2,axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = masks_all[100:]\n",
    "x_test = x_test.reshape ((x_test.shape[0],images_all.shape[1],images_all.shape[2],1))\n",
    "y_test = y_train2[100:]\n",
    "y_test = y_test.reshape ((y_test.shape[0],y_train2.shape[1],y_train2.shape[2],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Training a convolutional NN for the given dataset </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "configuration = tf.compat.v1.ConfigProto()\n",
    "configuration.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=configuration)\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(121, 121, 1)),\n",
    "  tf.keras.layers.AveragePooling2D((3, 3)),\n",
    "  tf.keras.layers.Conv2D(32, (3,3)),  \n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.AveragePooling2D((2, 2)),  \n",
    "  tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),  \n",
    "  tf.keras.layers.Flatten(),  \n",
    "  tf.keras.layers.Dense (121 * 121)              \n",
    "])\n",
    "model.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_split = 0.1, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> U-net model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Dataset preparation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape ((images_allH.shape[0],images_allH.shape[1] * images_allH.shape[2]))\n",
    "y_train = y_train.reshape ((images_allH.shape[0],y_train2.shape[1] * y_train2.shape[2]))\n",
    "scalerx = StandardScaler()\n",
    "scalery = StandardScaler()\n",
    "x_train = scalerx.fit_transform(x_train)\n",
    "y_train = scalery.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape ((images_allH.shape[0],images_allH.shape[1] * images_allH.shape[2]))\n",
    "y_train = y_train.reshape ((images_allH.shape[0],y_train2.shape[1] * y_train2.shape[2]))\n",
    "x_train = x_train.reshape ((images_allH.shape[0],images_allH.shape[1],images_allH.shape[2]))\n",
    "y_train = y_train.reshape ((images_allH.shape[0],y_train2.shape[1],y_train2.shape[2]))\n",
    "x_max = []\n",
    "y_max = []\n",
    "for i in range (images_allH.shape[0]):\n",
    "    x_max.append (x_train [i].max())\n",
    "for i in range (images_allH.shape[0]):\n",
    "    y_max.append (y_train [i].max())\n",
    "dimSize = 128\n",
    "x_train3 = np.zeros ((30,dimSize*dimSize))\n",
    "y_train3 = np.zeros ((30,dimSize*dimSize))\n",
    "x_test3 = np.zeros ((7,dimSize*dimSize))\n",
    "y_test3 = np.zeros ((7,dimSize*dimSize))\n",
    "for i in range (30):\n",
    "    x_train3[i,:] = x_train[i,:,:].reshape (dimSize * dimSize)\n",
    "    y_train3[i,:] = y_train[i,:,:].reshape (dimSize * dimSize)\n",
    "j = 0\n",
    "for i in range (31,37):\n",
    "    x_test3[j,:] = x_train[i,:,:].reshape (dimSize * dimSize)\n",
    "    y_test3[j,:] = y_train[i,:,:].reshape (dimSize * dimSize)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape ((images_all.shape[0],images_all.shape[1] * images_all.shape[2]))\n",
    "y_train = y_train.reshape ((images_all.shape[0],y_train2.shape[1] * y_train2.shape[2]))\n",
    "x_train_t = x_train.reshape ((images_all.shape[0],images_all.shape[1],images_all.shape[2]))\n",
    "y_train_t = y_train.reshape ((images_all.shape[0],y_train2.shape[1],y_train2.shape[2]))\n",
    "x_max = []\n",
    "y_max = []\n",
    "for i in range (images_all.shape[0]):\n",
    "    x_max.append (x_train_t [i].max())\n",
    "for i in range (images_all.shape[0]):\n",
    "    y_max.append (y_train_t [i].max())\n",
    "print (x_train_t.shape)\n",
    "print (y_train_t.shape)\n",
    "dimSize = 128\n",
    "x_train3 = np.zeros ((100,dimSize*dimSize))\n",
    "y_train3 = np.zeros ((100,dimSize*dimSize))\n",
    "\n",
    "x_test3 = np.zeros ((37,dimSize*dimSize),dtype = np.float)\n",
    "y_test3 = np.zeros ((37,dimSize*dimSize),dtype = np.float)\n",
    "\n",
    "for i in range (100):\n",
    "    x_train3[i,:] = x_train_t[i,:,:].reshape (dimSize * dimSize)\n",
    "    y_train3[i,:] = y_train_t[i,:,:].reshape (dimSize * dimSize)\n",
    "j = 0\n",
    "for i in range (101,137):\n",
    "    x_test3[j,:] = x_train_t[i,:,:].reshape (dimSize * dimSize)\n",
    "    y_test3[j,:] = y_train_t[i,:,:].reshape (dimSize * dimSize)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape ((images_all.shape[0],images_all.shape[1] * images_all.shape[2]))\n",
    "y_train = y_train.reshape ((images_all.shape[0],y_train2.shape[1] * y_train2.shape[2]))\n",
    "x_train = x_train.reshape ((images_all.shape[0],images_all.shape[1],images_all.shape[2]))\n",
    "y_train = y_train.reshape ((images_all.shape[0],y_train2.shape[1],y_train2.shape[2]))\n",
    "x_max = []\n",
    "y_max = []\n",
    "dimSize = 128\n",
    "x_train3 = np.zeros ((100,dimSize*dimSize))\n",
    "y_train3 = np.zeros ((100,dimSize*dimSize))\n",
    "\n",
    "x_test3 = np.zeros ((15,dimSize*dimSize),dtype = np.float)\n",
    "y_test3 = np.zeros ((15,dimSize*dimSize),dtype = np.float)\n",
    "\n",
    "for i in range (100):\n",
    "    x_train3[i,:] = x_train[i,:,:].reshape (dimSize * dimSize)\n",
    "    y_train3[i,:] = y_train[i,:,:].reshape (dimSize * dimSize)\n",
    "j = 0\n",
    "for i in range (101,115):\n",
    "    x_test3[j,:] = x_train[i,:,:].reshape (dimSize * dimSize)\n",
    "    y_test3[j,:] = y_train[i,:,:].reshape (dimSize * dimSize)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Training U-net model </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "\n",
    "\n",
    "def unet(optimizer = 'Adam', pretrained_weights = None,input_size = (128,128,1)):\n",
    "    inputs = tf.keras.Input(input_size)\n",
    "    conv1 = tf.keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = tf.keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = tf.keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = tf.keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = tf.keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = tf.keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = tf.keras.layers.Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = tf.keras.layers.Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = tf.keras.layers.Dropout(0.25)(conv4)\n",
    "    pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = tf.keras.layers.Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = tf.keras.layers.Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = tf.keras.layers.Dropout(0.25)(conv5)\n",
    "\n",
    "    up6 = tf.keras.layers.Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(tf.keras.layers.UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = tf.keras.layers.concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = tf.keras.layers.Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = tf.keras.layers.Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = tf.keras.layers.Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(tf.keras.layers.UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = tf.keras.layers.concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = tf.keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = tf.keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = tf.keras.layers.Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(tf.keras.layers.UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = tf.keras.layers.concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = tf.keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = tf.keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = tf.keras.layers.Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(tf.keras.layers.UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = tf.keras.layers.concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = tf.keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = tf.keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = tf.keras.layers.Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = tf.keras.layers.Conv2D(1, 1, activation = 'linear')(conv9)\n",
    "\n",
    "    model = tf.keras.Model(inputs, conv10)\n",
    "\n",
    "    model.compile(optimizer = optimizer, loss = 'mse', metrics = ['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "model = KerasClassifier(build_fn=unet, epochs=70, batch_size=1, verbose=0)\n",
    "optimizer = ['SGD', 'RMSprop', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize*dimSize)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = []\n",
    "initial_lr = 0.00001\n",
    "for i in range (7):\n",
    "    lrs.append (initial_lr)\n",
    "    initial_lr = initial_lr * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "lrs = []\n",
    "initial_lr = 0.00001\n",
    "for i in range (7):\n",
    "    lrs.append (initial_lr)\n",
    "    initial_lr = initial_lr * 0.1\n",
    "for i in range (7):\n",
    "    model = unet()\n",
    "    model.summary()\n",
    "    loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = lrs [i])\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=loss_fn,\n",
    "                  metrics=['mse'])\n",
    "    history = model.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=100)\n",
    "    models.append (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.history import HistoryAccessor\n",
    "hista = HistoryAccessor()\n",
    "z1 = hista.search('*images_all = np.array*')\n",
    "z1.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (x_train3[10].shape)\n",
    "xd = x_train3[10].reshape (128,128)\n",
    "plt.imshow (xd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Finding best model hyper-parameters for HI </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet()\n",
    "model.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.00001)\n",
    "model.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('models/unet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = load_model ('C:/Users/NU/Documents/dana_collaboration_project/models/model_8_H_dataset/saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = unet()\n",
    "model2.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Nadam(\n",
    "    learning_rate=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n",
    ")\n",
    "model2.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "history = model2.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = unet()\n",
    "model3.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\",\n",
    ")\n",
    "model3.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "history = model3.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = unet()\n",
    "model4.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.00001#, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n",
    ")\n",
    "model4.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "history = model4.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = unet()\n",
    "model5.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.00001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model5.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model5.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = unet()\n",
    "model7.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.000001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model7.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model7.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = unet()\n",
    "model8.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.0001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model8.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model8.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = unet()\n",
    "model9.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model9.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model9.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = unet()\n",
    "model10.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.0001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "model10.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "history = model10.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11 = unet()\n",
    "model11.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.0001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model11.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model11.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model12 = unet()\n",
    "model12.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.00001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model12.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model12.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model13 = unet()\n",
    "model13.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.01,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model13.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model13.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model14 = unet()\n",
    "model14.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.008,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model14.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model14.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15 = unet()\n",
    "model15.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.0001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model15.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model15.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h = unet()\n",
    "model_h.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.00001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model_h.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hh = unet()\n",
    "model_hh.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.0001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model_hh.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hh.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hhh = unet()\n",
    "model_hhh.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.00001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model_hhh.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hhh.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h4 = unet()\n",
    "model_h4.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.00003,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model_h4.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h4.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h5 = unet()\n",
    "model_h5.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.000001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model_h5.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h5.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h6 = unet()\n",
    "model_h6.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.00001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model_h6.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h6.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h7 = unet()\n",
    "model_h7.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "model_h7.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h7.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h8 = unet()\n",
    "model_h8.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.001, rho=0.95, epsilon=1e-07, name=\"Adadelta\"\n",
    ")\n",
    "\n",
    "model_h8.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h8.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h9 = unet()\n",
    "model_h9.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.0001, rho=0.95, epsilon=1e-07, name=\"Adadelta\"\n",
    ")\n",
    "\n",
    "model_h9.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h9.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h11.save ('models/model_adadelta_lr003.h5')\n",
    "saved_m = unet()\n",
    "saved_m.summary()\n",
    "saved_m.load_weights ('models/model_adadelta_lr003.h5')\n",
    "saved_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h10 = unet()\n",
    "model_h10.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.05, rho=0.95, epsilon=1e-07, name=\"Adadelta\"\n",
    ")\n",
    "\n",
    "model_h10.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h10.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_h11 = unet()\n",
    "model_h11.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.03, rho=0.95, epsilon=1e-07, name=\"Adadelta\"\n",
    ")\n",
    "\n",
    "model_h11.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h11.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h12 = unet()\n",
    "model_h12.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.04, rho=0.95, epsilon=1e-07, name=\"Adadelta\"\n",
    ")\n",
    "\n",
    "model_h12.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h12.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h12.save ('models/model_adadelta_lr004.h5')\n",
    "saved_m = unet()\n",
    "saved_m.summary()\n",
    "saved_m.load_weights ('models/model_adadelta_lr004.h5')\n",
    "saved_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_m = unet()\n",
    "saved_m.summary()\n",
    "saved_m.load_weights ('models/model_adam_lr00005.h5')\n",
    "saved_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h13 = unet()\n",
    "model_h13.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"\n",
    ")\n",
    "\n",
    "\n",
    "model_h13.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h13.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h14 = unet()\n",
    "model_h14.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"\n",
    ")\n",
    "\n",
    "model_h14.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h14.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h16.save ('models/model_adamax_lr00001.h5')\n",
    "saved_m = unet()\n",
    "saved_m.summary()\n",
    "saved_m.load_weights ('models/model_adamax_lr00001.h5')\n",
    "saved_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h14 = unet()\n",
    "model_h14.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.00005, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"\n",
    ")\n",
    "\n",
    "model_h14.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h14.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h14.save ('models/model_adamax_lr00005.h5')\n",
    "saved_m = unet()\n",
    "saved_m.summary()\n",
    "saved_m.load_weights ('models/model_adamax_lr00005.h5')\n",
    "saved_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h15 = unet()\n",
    "model_h15.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.00005, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"\n",
    ")\n",
    "\n",
    "model_h15.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h15.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h16 = unet()\n",
    "model_h16.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.00005)#, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\",)\n",
    "\n",
    "model_h16.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h16.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h16.save ('models/model_adam_lr00005.h5')\n",
    "saved_m = unet()\n",
    "saved_m.summary()\n",
    "saved_m.load_weights ('models/model_adam_lr00005.h5')\n",
    "saved_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h17 = unet()\n",
    "model_h17.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)#, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\",)\n",
    "\n",
    "model_h17.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h17.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h17.save ('models/model_adam_lr0001.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h17.save ('models/model_adam_lr0001.h5')\n",
    "saved_m = unet()\n",
    "saved_m.summary()\n",
    "saved_m.load_weights ('models/model_adam_lr0001.h5')\n",
    "saved_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h18 = unet()\n",
    "model_h18.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0002)#, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\",)\n",
    "\n",
    "model_h18.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h18.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h19 = unet()\n",
    "model_h19.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.004)#, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\",)\n",
    "\n",
    "\n",
    "model_h19.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h19.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3> Finding best model hyper-parameters for Herschel dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel = unet()\n",
    "model_hershel.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.042, rho=0.95, epsilon=1e-07, name=\"Adadelta\"\n",
    ")\n",
    "model_hershel.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel1 = unet()\n",
    "model_hershel1.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.00005, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"\n",
    ")\n",
    "\n",
    "model_hershel1.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel1.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel2 = unet()\n",
    "model_hershel2.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"\n",
    ")\n",
    "\n",
    "model_hershel2.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel2.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel3 = unet()\n",
    "model_hershel3.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "\n",
    "model_hershel3.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel3.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel4 = unet()\n",
    "model_hershel4.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.00001)\n",
    "\n",
    "model_hershel4.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel4.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel5 = unet()\n",
    "model_hershel5.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.0004, rho=0.95, epsilon=1e-07, name=\"Adadelta\"\n",
    ")\n",
    "\n",
    "model_hershel5.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel5.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel6 = unet()\n",
    "model_hershel6.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.004, rho=0.95, epsilon=1e-07, name=\"Adadelta\"\n",
    ")\n",
    "\n",
    "model_hershel6.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel6.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel7 = unet()\n",
    "model_hershel7.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "\n",
    "model_hershel7.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel7.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel8 = unet()\n",
    "model_hershel8.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.00001)\n",
    "\n",
    "model_hershel8.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel8.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel9 = unet()\n",
    "model_hershel9.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.004,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model_hershel9.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel9.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel10 = unet()\n",
    "model_hershel10.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"\n",
    ")\n",
    "\n",
    "model_hershel10.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel10.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel11 = unet()\n",
    "model_hershel11.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"\n",
    ")\n",
    "\n",
    "model_hershel11.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel11.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel12 = unet()\n",
    "model_hershel12.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"\n",
    ")\n",
    "\n",
    "model_hershel12.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel12.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel12 = unet()\n",
    "model_hershel12.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(\n",
    "    learning_rate=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n",
    ")\n",
    "\n",
    "model_hershel12.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel12.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel13 = unet()\n",
    "model_hershel13.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.0007)\n",
    "\n",
    "model_hershel13.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel13.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel14 = unet()\n",
    "model_hershel14.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.000007)\n",
    "\n",
    "model_hershel14.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel14.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel15 = unet()\n",
    "model_hershel15.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adamax(\n",
    "    learning_rate=0.000001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\"\n",
    ")\n",
    "\n",
    "model_hershel15.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel15.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel16 = unet()\n",
    "model_hershel16.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.1,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model_hershel16.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel16.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10):\n",
    "    '''\n",
    "    Wrapper function to create a LearningRateScheduler with step decay schedule.\n",
    "    '''\n",
    "    def schedule(epoch):\n",
    "        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n",
    "    \n",
    "    return LearningRateScheduler(schedule)\n",
    "\n",
    "lr_sched = step_decay_schedule(initial_lr=1e-4, decay_factor=0.75, step_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hershel17 = unet()\n",
    "model_hershel17.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.0004)\n",
    "\n",
    "model_hershel17.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_hershel17.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Finding best model hyper-parameters for Planck dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p = unet()\n",
    "model_p.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.00005)\n",
    "\n",
    "model_p.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p = unet()\n",
    "model_p.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model_p.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p1 = unet()\n",
    "model_p1.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.002)\n",
    "\n",
    "model_p1.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p1.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p2 = unet()\n",
    "model_p2.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(\n",
    "    learning_rate=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n",
    ")\n",
    "\n",
    "model_p2.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p2.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p3 = unet()\n",
    "model_p3.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.00001,\n",
    "    rho=0.9,\n",
    "    momentum=0.0,\n",
    "    epsilon=1e-07,\n",
    "    centered=False,\n",
    "    name=\"RMSprop\"\n",
    ")\n",
    "model_p3.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p3.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p4 = unet()\n",
    "model_p4.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.00002,\n",
    "    rho=0.9,\n",
    "    momentum=0.0,\n",
    "    epsilon=1e-07,\n",
    "    centered=False,\n",
    "    name=\"RMSprop\"\n",
    ")\n",
    "model_p4.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p4.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p5 = unet()\n",
    "model_p5.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(\n",
    "    learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n",
    ")\n",
    "\n",
    "model_p5.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p5.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p6 = unet()\n",
    "model_p6.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(\n",
    "    learning_rate=0.00003, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n",
    ")\n",
    "\n",
    "model_p6.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p6.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p7 = unet()\n",
    "model_p7.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(\n",
    "    learning_rate=0.00003, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n",
    ")\n",
    "\n",
    "model_p7.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p7.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p8 = unet()\n",
    "model_p8.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(\n",
    "    learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n",
    ")\n",
    "\n",
    "model_p8.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p8.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p9 = unet()\n",
    "model_p9.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.00003)\n",
    "\n",
    "model_p9.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p9.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p10 = unet()\n",
    "model_p10.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.00007)\n",
    "\n",
    "model_p10.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p10.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p11 = unet()\n",
    "model_p11.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.00005)\n",
    "\n",
    "model_p11.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p11.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p12 = unet()\n",
    "model_p12.summary()\n",
    "#Logarithmic\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.0005)\n",
    "\n",
    "model_p12.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p12.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p13 = unet()\n",
    "model_p13.summary()\n",
    "#Logarithmic\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.000001)\n",
    "\n",
    "model_p13.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p13.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p14 = unet()\n",
    "model_p14.summary()\n",
    "#Logarithmic\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "\n",
    "model_p14.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p14.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p15 = unet()\n",
    "model_p15.summary()\n",
    "#Logarithmic\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "model_p15.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p15.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p16 = unet()\n",
    "model_p16.summary()\n",
    "#Logarithmic\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(\n",
    "    learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n",
    ")\n",
    "\n",
    "model_p16.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p16.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p17 = unet()\n",
    "model_p17.summary()\n",
    "#Logarithmic\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.0001, rho=0.95, epsilon=1e-07, name=\"Adadelta\"\n",
    ")\n",
    "\n",
    "model_p17.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p17.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p17 = unet()\n",
    "model_p17.summary()\n",
    "#Logarithmic\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.00003)\n",
    "\n",
    "model_p17.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_p17.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h20 = unet()\n",
    "model_h20.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.0001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model_h20.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_h20.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=True,\n",
    "    name=\"Adam\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model16 = unet()\n",
    "model16.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.00001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model16.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model16.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model17 = unet()\n",
    "model17.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.000001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model17.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model17.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_H = unet()\n",
    "model_H.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.000003)#, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Adamax\",)\n",
    "\n",
    "model_H.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_H.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_H3 = unet()\n",
    "model_H3.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.000001)\n",
    "\n",
    "model_H3.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model_H3.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model18 = unet()\n",
    "model18.summary()\n",
    "loss_fn = tf.keras.losses.MeanSquaredLogarithmicError(reduction=tf.keras.losses.Reduction.SUM) # \n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "     learning_rate=0.0000001,\n",
    "     initial_accumulator_value=0.1,\n",
    "     epsilon=1e-07,\n",
    "     name=\"Adagrad\")\n",
    "\n",
    "model18.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse'])\n",
    "\n",
    "history = model18.fit(x_train3.reshape ((x_train3.shape[0],dimSize,dimSize,1)), y_train3.reshape ((x_train3.shape[0],dimSize,dimSize)), validation_split = 0.1, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map1 = fits.open ('figures/unet/theta_1.fits')\n",
    "map2 = fits.open ('figures/unet/unet_1.fits')\n",
    "map3 = fits.open ('figures/unet/intensity_1.fits')\n",
    "map1 = map1[0].data\n",
    "map2 = map2[0].data\n",
    "map3 = map3[0].data\n",
    "\n",
    "f, axarr = plt.subplots(1,3) \n",
    "axarr[0].axis('off')\n",
    "axarr[0].imshow(map1, cmap = 'Wistia')\n",
    "#axarr[0].set_title('rht theta')\n",
    "plt.axis('off')\n",
    "axarr[1].imshow(map2, cmap = 'Wistia')\n",
    "axarr[1].axis('off')\n",
    "axarr[2].imshow( map3, cmap = 'RdBu_r')\n",
    "axarr[2].axis('off')\n",
    "#axarr[2].set_title('intensity map')\n",
    "plt.savefig('figures/unet/figure5/map_paper' + '.png',\n",
    "        bbox_inches =\"tight\",\n",
    "        pad_inches = 1,\n",
    "        facecolor =\"w\",\n",
    "        edgecolor ='w',\n",
    "        dpi = 300,    \n",
    "        orientation ='landscape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Test a model on a sample map G202.85 from Herschel dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = fits.open ('example_map.fits')\n",
    "map_data = mp[0].data\n",
    "xd = np.expand_dims(map_data.reshape (dimSize,dimSize,1), axis=0)\n",
    "data = model4.predict(xd)\n",
    "plt.show (data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
